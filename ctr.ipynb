{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepfm\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, field_size, embedding_size]\n",
    "    Output shape\n",
    "    - [batch_size, 1]\n",
    "\"\"\"\n",
    "\n",
    "class FM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(FM, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = tf.square(tf.reduce_sum(\n",
    "            concated_embeds_value, axis=1, keep_dims=True))\n",
    "        sum_of_square = tf.reduce_sum(\n",
    "            concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)\n",
    "        cross_term = square_of_sum - sum_of_square\n",
    "        cross_term = 0.5 * tf.reduce_sum(cross_term, axis=2, keep_dims=False)\n",
    "        return cross_term\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent deepfm \n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, field_size, embedding_size]\n",
    "    Output shape\n",
    "    - [batch_size, 1]\n",
    "\"\"\"\n",
    "\n",
    "class FM(Layer):\n",
    "    def __init__(self, k=10, **kwargs):\n",
    "        self.k = k\n",
    "        super(FM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = int(input_shape[-1])\n",
    "        self.V = self.add_weight(name='V', \n",
    "                                 shape=(self.k, dim),\n",
    "                                 initializer=glorot_normal(),\n",
    "                                 trainable=True)\n",
    "        super(FM, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        second_order = 0.5 * tf.reduce_sum(\n",
    "            tf.square(tf.matmul(inputs, tf.transpose(self.V))) -\n",
    "            tf.matmul(tf.square(inputs), tf.square(tf.transpose(self.V))), \n",
    "            axis=2, keepdims=False)\n",
    "        return second_order\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fwfm\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, field_size, embedding_size]\n",
    "    Output shape\n",
    "    - [batch_size, 1]\n",
    "\"\"\"\n",
    "import itertools\n",
    "\n",
    "class FwFM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FwFM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_fields = int(input_shape[1])\n",
    "        self.field_strengths = self.add_weight(name='field_pair_weights',\n",
    "                                               shape=(self.num_fields, self.num_fields),\n",
    "                                               initializer=glorot_normal(),\n",
    "                                               trainable=True)\n",
    "\n",
    "        super(FwFM, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pairwise_inner_prods = []\n",
    "        for fi, fj in itertools.combinations(range(self.num_fields), 2):\n",
    "            r_ij = self.field_strengths[fi, fj]\n",
    "            feat_embed_i = tf.squeeze(inputs[:, fi:fi + 1, :], axis=1)\n",
    "            feat_embed_j = tf.squeeze(inputs[:, fj:fj + 1, :], axis=1)\n",
    "\n",
    "            f = tf.scalar_mul(r_ij, K.batch_dot(feat_embed_i, feat_embed_j, axes=1))\n",
    "            pairwise_inner_prods.append(f)\n",
    "\n",
    "        sum_ = tf.add_n(pairwise_inner_prods)\n",
    "        return sum_\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPNN\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - A list of [batch_size, 1, embedding_size]\n",
    "    Output shape\n",
    "    - [batch_size, N * (N - 1) / 2, 1]\n",
    "\"\"\"\n",
    "\n",
    "class InnerProductLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InnerProductLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(InnerProductLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        embed_list = inputs\n",
    "        row = []\n",
    "        col = []\n",
    "        num_inputs = len(embed_list)\n",
    "\n",
    "        for i in range(num_inputs - 1):\n",
    "            for j in range(i + 1, num_inputs):\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "        p = tf.concat([embed_list[idx] for idx in row], axis=1)\n",
    "        q = tf.concat([embed_list[idx] for idx in col], axis=1)\n",
    "\n",
    "        inner_product = p * q\n",
    "        inner_product = tf.reduce_sum(inner_product, axis=2, keep_dims=True)\n",
    "        return inner_product\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        num_inputs = len(input_shape)\n",
    "        num_pairs = int(num_inputs * (num_inputs - 1) / 2)\n",
    "        input_shape = input_shape[0]\n",
    "        embed_size = input_shape[-1]\n",
    "        return (input_shape[0], num_pairs, 1)\n",
    "    \n",
    "    \n",
    "# OPNN\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - A list of [batch_size, 1, embedding_size]\n",
    "\n",
    "    Output shape\n",
    "    - [batch_size, N * (N - 1) / 2]\n",
    "\"\"\"\n",
    "\n",
    "class OutterProductLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(OutterProductLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_inputs = len(input_shape)\n",
    "        num_pairs = int(num_inputs * (num_inputs - 1) / 2)\n",
    "        input_shape = input_shape[0]\n",
    "        embed_size = int(input_shape[-1])\n",
    "        self.kernel = self.add_weight(shape=(embed_size, num_pairs, embed_size),\n",
    "                                      initializer=glorot_uniform(),\n",
    "                                      name='kernel')\n",
    "        super(OutterProductLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        embed_list = inputs\n",
    "        row = []\n",
    "        col = []\n",
    "        num_inputs = len(embed_list)\n",
    "        for i in range(num_inputs - 1):\n",
    "            for j in range(i + 1, num_inputs):\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "        p = tf.concat([embed_list[idx] for idx in row], axis=1)\n",
    "        q = tf.concat([embed_list[idx] for idx in col], axis=1)\n",
    "        p = tf.expand_dims(p, 1)\n",
    "        kp = tf.reduce_sum(tf.multiply(tf.transpose(tf.reduce_sum(tf.multiply(p, self.kernel), -1), [0, 2, 1]), q), -1)\n",
    "        return kp\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        num_inputs = len(input_shape)\n",
    "        num_pairs = int(num_inputs * (num_inputs - 1) / 2)\n",
    "        return (None, num_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCN_V1\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, feature_dims]\n",
    "    Output shape\n",
    "    - [batch_size, feature_dims]\n",
    "\"\"\"\n",
    "\n",
    "class CrossNet_V1(Layer):\n",
    "    def __init__(self, layer_num=2, **kwargs):\n",
    "        self.layer_num = layer_num\n",
    "        super(CrossNet, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = int(input_shape[-1])\n",
    "        self.kernels = [self.add_weight(name='kernel' + str(i),\n",
    "                                        shape=(dim, 1),\n",
    "                                        initializer=glorot_normal(),\n",
    "                                        trainable=True) for i in range(self.layer_num)]\n",
    "        self.bias = [self.add_weight(name='bias' + str(i),\n",
    "                                     shape=(dim, 1),\n",
    "                                     initializer=Zeros(),\n",
    "                                     trainable=True) for i in range(self.layer_num)]\n",
    "        super(CrossNet, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x_0 = tf.expand_dims(inputs, axis=2)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            xl_w = tf.tensordot(x_l, self.kernels[i], axes=(1, 0))\n",
    "            dot_ = tf.matmul(x_0, xl_w)\n",
    "            x_l = dot_ + self.bias[i] + x_l\n",
    "        x_l = tf.squeeze(x_l, axis=2)\n",
    "        return x_l\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'layer_num': self.layer_num}\n",
    "        base_config = super(CrossNet, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCN_V2\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, feature_dims]\n",
    "    Output shape\n",
    "    - [batch_size, feature_dims]\n",
    "\"\"\"\n",
    "\n",
    "class CrossNet_V2(Layer):\n",
    "    def __init__(self, layer_num=2, **kwargs):\n",
    "        self.layer_num = layer_num\n",
    "        super(CrossNet_V2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = int(input_shape[-1])\n",
    "        self.kernels = [self.add_weight(name='kernel' + str(i),\n",
    "                                        shape=(dim, dim),\n",
    "                                        initializer=glorot_normal(),\n",
    "                                        trainable=True) for i in range(self.layer_num)]\n",
    "        self.bias = [self.add_weight(name='bias' + str(i),\n",
    "                                     shape=(dim, 1),\n",
    "                                     initializer=Zeros(),\n",
    "                                     trainable=True) for i in range(self.layer_num)]\n",
    "        super(CrossNet_V2, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x_0 = tf.expand_dims(inputs, axis=2)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            wl_xl = tf.matmul(self.kernels[i], x_l)\n",
    "            x_m = wl_xl + self.bias[i]\n",
    "            x_l = x_0 * x_m + x_l\n",
    "        x_l = tf.squeeze(x_l, axis=2)\n",
    "        return x_l\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'layer_num': self.layer_num}\n",
    "        base_config = super(CrossNet_V2, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCN_M\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, feature_dims]\n",
    "    Output shape\n",
    "    - [batch_size, feature_dims]\n",
    "\"\"\"\n",
    "\n",
    "class CrossNetMix(Layer):\n",
    "    def __init__(self, low_rank=32, num_experts=4, layer_num=2):\n",
    "        self.low_rank = low_rank\n",
    "        self.num_experts = num_experts\n",
    "        self.layer_num = layer_num\n",
    "        super(CrossNetMix, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # U: (feature_dims, low_rank)\n",
    "        self.feature_dims = int(input_shape[1])\n",
    "        self.U_list = [self.add_weight(name='kernel_U' + str(i),\n",
    "                                       shape=(self.num_experts, self.feature_dims, self.low_rank),\n",
    "                                       initializer=glorot_normal())\n",
    "                                       for i in range(self.layer_num)]\n",
    "        # V: (feature_dims, low_rank)\n",
    "        self.V_list = [self.add_weight(name='kernel_V' + str(i),\n",
    "                                       shape=(self.num_experts, self.feature_dims, self.low_rank),\n",
    "                                       initializer=glorot_normal())\n",
    "                                       for i in range(self.layer_num)]\n",
    "\n",
    "        # C: (low_rank, low_rank)\n",
    "        self.C_list = [self.add_weight(name='kernel_C' + str(i),\n",
    "                                       shape=(self.num_experts, self.low_rank, self.low_rank),\n",
    "                                       initializer=glorot_normal())\n",
    "                                       for i in range(self.layer_num)]\n",
    "\n",
    "        self.gating = [Dense(1, use_bias=False) for i in range(self.num_experts)]\n",
    "\n",
    "        self.bias = [self.add_weight(name='bias' + str(i),\n",
    "                                    shape=(self.feature_dims, 1),\n",
    "                                    initializer=Zeros()) \n",
    "                                    for i in range(self.layer_num)]\n",
    "        super(CrossNetMix, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # (bs, feature_dims, 1)\n",
    "        x_0 = tf.expand_dims(inputs, 2)  \n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            output_of_experts = []\n",
    "            gating_score_of_experts = []\n",
    "            for expert_id in range(self.num_experts):\n",
    "                # (bs, feature_dims) -> (bs, 1)\n",
    "                gating_score_of_experts.append(self.gating[expert_id](tf.squeeze(x_l, 2)))\n",
    "\n",
    "                # (bs, feature_dims, 1) -> (bs, low_rank, 1)\n",
    "                v_x = tf.matmul(tf.transpose(self.V_list[i][expert_id]), x_l)\n",
    "\n",
    "                v_x = tf.tanh(v_x)\n",
    "                \n",
    "                # (bs, low_rank, 1) -> (bs, low_rank, 1)\n",
    "                v_x = tf.matmul(self.C_list[i][expert_id], v_x)\n",
    "                v_x = tf.tanh(v_x)\n",
    "\n",
    "                # (bs, low_rank, 1) -> (bs, feature_dims, 1)\n",
    "                uv_x = tf.matmul(self.U_list[i][expert_id], v_x)  \n",
    "\n",
    "                dot_ = uv_x + self.bias[i]\n",
    "                \n",
    "                # (bs, feature_dims, 1)\n",
    "                dot_ = x_0 * dot_ \n",
    "\n",
    "                output_of_experts.append(tf.squeeze(dot_, 2))\n",
    "\n",
    "            # (bs, feature_dims, num_experts)\n",
    "            output_of_experts = tf.stack(output_of_experts, 2)  \n",
    "            # (bs, num_experts, 1)\n",
    "            gating_score_of_experts = tf.stack(gating_score_of_experts, 1)  \n",
    "            gating_value = tf.nn.softmax(gating_score_of_experts, 1)\n",
    "            \n",
    "            moe_out = tf.matmul(output_of_experts, gating_value)\n",
    "            x_l = moe_out + x_l  # (bs, feature_dims, 1)\n",
    "\n",
    "        x_l = tf.squeeze(x_l, -1)  # (bs, feature_dims)\n",
    "        return x_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XDeepFM\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, field_nums, embed_dims]\n",
    "    Output shape\n",
    "    - [batch_size, featuremap_num]\n",
    "\"\"\"\n",
    "\n",
    "class CIN(Layer):\n",
    "    def __init__(self, layer_size=(128, 128), activation='relu', split_half=True, **kwargs):\n",
    "        self.layer_size = layer_size\n",
    "        self.split_half = split_half\n",
    "        self.activation = activation\n",
    "        super(CIN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_nums = [int(input_shape[1])]\n",
    "        self.filters = []\n",
    "        self.bias = []\n",
    "        for i, size in enumerate(self.layer_size):\n",
    "            self.filters.append(self.add_weight(name='filter' + str(i),\n",
    "                                                shape=[1, self.field_nums[-1] * self.field_nums[0], size],\n",
    "                                                dtype=tf.float32, \n",
    "                                                initializer=glorot_uniform()))\n",
    "            self.bias.append(self.add_weight(name='bias' + str(i), \n",
    "                                             shape=[size], \n",
    "                                             dtype=tf.float32,\n",
    "                                             initializer=tf.keras.initializers.Zeros()))\n",
    "            if self.split_half:\n",
    "                self.field_nums.append(size // 2)\n",
    "            else:\n",
    "                self.field_nums.append(size)\n",
    "        self.activation_layers = [Activation(self.activation) for _ in self.layer_size]\n",
    "        super(CIN, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        dim = int(inputs.get_shape()[-1])\n",
    "        hidden_nn_layers = [inputs]\n",
    "        final_result = []\n",
    "        split_tensor0 = tf.split(hidden_nn_layers[0], dim * [1], 2)\n",
    "        \n",
    "        for idx, layer_size in enumerate(self.layer_size):\n",
    "            split_tensor = tf.split(hidden_nn_layers[-1], dim * [1], 2)\n",
    "            \n",
    "            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n",
    "            \n",
    "            dot_result_o = tf.reshape(dot_result_m, shape=[dim, -1, self.field_nums[0] * self.field_nums[idx]])\n",
    "            \n",
    "            dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])\n",
    "            \n",
    "            curr_out = tf.nn.conv1d(dot_result, filters=self.filters[idx], stride=1, padding='VALID')\n",
    "            curr_out = tf.nn.bias_add(curr_out, self.bias[idx])\n",
    "            curr_out = self.activation_layers[idx](curr_out)\n",
    "            curr_out = tf.transpose(curr_out, perm=[0, 2, 1])\n",
    "\n",
    "            if self.split_half:\n",
    "                if idx != len(self.layer_size) - 1:\n",
    "                    next_hidden, direct_connect = tf.split(curr_out, 2 * [layer_size // 2], 1)\n",
    "                else:\n",
    "                    direct_connect = curr_out\n",
    "                    next_hidden = 0\n",
    "            else:\n",
    "                direct_connect = curr_out\n",
    "                next_hidden = curr_out\n",
    "                \n",
    "            final_result.append(direct_connect)\n",
    "            hidden_nn_layers.append(next_hidden)\n",
    "            \n",
    "        result = tf.concat(final_result, axis=1)\n",
    "        result = tf.reduce_sum(result, -1, keep_dims=False)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.split_half:\n",
    "            featuremap_num = sum(self.layer_size[:-1]) // 2 + self.layer_size[-1]\n",
    "        else:\n",
    "            featuremap_num = sum(self.layer_size)\n",
    "        return (None, featuremap_num)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'layer_size': self.layer_size, 'split_half': self.split_half, 'activation': self.activation}\n",
    "        base_config = super(CIN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fibinet\n",
    "\n",
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - A list of [batch_size, 1, embed_dims]\n",
    "    Output shape\n",
    "    - A list of [batch_size, 1, embed_dims]\n",
    "\"\"\"\n",
    "\n",
    "class SENETLayer(Layer):\n",
    "    def __init__(self, reduction_ratio=3, **kwargs):\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        super(SENETLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_size = int(len(input_shape))\n",
    "        self.embedding_size = int(input_shape[0][-1])\n",
    "        reduction_size = max(1, self.field_size // self.reduction_ratio)\n",
    "\n",
    "        self.W_1 = self.add_weight(shape=(\n",
    "            self.field_size, reduction_size), initializer=glorot_normal(), name=\"W_1\")\n",
    "        self.W_2 = self.add_weight(shape=(\n",
    "            reduction_size, self.field_size), initializer=glorot_normal(), name=\"W_2\")\n",
    "        self.tensordot = tf.keras.layers.Lambda(\n",
    "            lambda x: tf.tensordot(x[0], x[1], axes=(-1, 0)))\n",
    "        \n",
    "        super(SENETLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        inputs = tf.concat(inputs, axis=1)\n",
    "        Z = tf.reduce_mean(inputs, axis=-1)\n",
    "\n",
    "        A_1 = tf.nn.relu(self.tensordot([Z, self.W_1]))\n",
    "        A_2 = tf.nn.relu(self.tensordot([A_1, self.W_2]))\n",
    "        V = tf.multiply(inputs, tf.expand_dims(A_2, axis=2))\n",
    "\n",
    "        return tf.split(V, self.field_size, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return input_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return [None] * self.field_size\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'reduction_ratio': self.reduction_ratio}\n",
    "        base_config = super(SENETLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "\n",
    "# BilinearInteraction\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - A list of [batch_size, 1, embed_dims]\n",
    "    Output shape\n",
    "    - [batch_size, 1, field_nums * (field_nums - 1) // 2 * embed_dims)]\n",
    "\"\"\"\n",
    "\n",
    "class BilinearInteraction(Layer):\n",
    "    def __init__(self, bilinear_type=\"interaction\", **kwargs):\n",
    "        self.bilinear_type = bilinear_type\n",
    "        super(BilinearInteraction, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_size = int(input_shape[0][-1])\n",
    "        if self.bilinear_type == \"all\":\n",
    "            self.W = self.add_weight(shape=(embedding_size, embedding_size),\n",
    "                                     initializer=glorot_normal(),\n",
    "                                     name=\"bilinear_weight\")\n",
    "        elif self.bilinear_type == \"each\":\n",
    "            self.W_list = [self.add_weight(shape=(embedding_size, embedding_size),\n",
    "                                           initializer=glorot_normal(),\n",
    "                                           name=\"bilinear_weight\" + str(i))\n",
    "                           for i in range(len(input_shape) - 1)]\n",
    "        elif self.bilinear_type == \"interaction\":\n",
    "            self.W_list = [self.add_weight(shape=(embedding_size, embedding_size),\n",
    "                                           initializer=glorot_normal(),\n",
    "                                           name=\"bilinear_weight\" + str(i) + '_' + str(j))\n",
    "                           for i, j in itertools.combinations(range(len(input_shape)), 2)]\n",
    "        super(BilinearInteraction, self).build(input_shape)  \n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.bilinear_type == \"all\":\n",
    "            p = [tf.multiply(tf.tensordot(v_i, self.W, axes=(-1, 0)), v_j)\n",
    "                 for v_i, v_j in itertools.combinations(inputs, 2)]\n",
    "        elif self.bilinear_type == \"each\":\n",
    "            p = [tf.multiply(tf.tensordot(inputs[i], self.W_list[i], axes=(-1, 0)), inputs[j])\n",
    "                 for i, j in itertools.combinations(range(len(inputs)), 2)]\n",
    "        elif self.bilinear_type == \"interaction\":\n",
    "            p = [tf.multiply(tf.tensordot(v[0], w, axes=(-1, 0)), v[1])\n",
    "                 for v, w in zip(itertools.combinations(inputs, 2), self.W_list)]\n",
    "        return tf.concat(p, axis=-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        filed_size = len(input_shape)\n",
    "        embedding_size = input_shape[0][-1]\n",
    "        return (None, 1, filed_size * (filed_size - 1) // 2 * embedding_size)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'bilinear_type': self.bilinear_type}\n",
    "        base_config = super(BilinearInteraction, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgcnn\n",
    "\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, field_nums, embedding_dims]\n",
    "\n",
    "    Output shape\n",
    "    - [batch_size, new_feature_nums, embedding_dims]\n",
    "\"\"\"\n",
    "\n",
    "class FGCNNLayer(Layer):\n",
    "    def __init__(self, filters=(14, 16,), kernel_width=(7, 7,), new_maps=(3, 3,), pooling_width=(2, 2), **kwargs):\n",
    "        self.filters = filters\n",
    "        self.kernel_width = kernel_width\n",
    "        self.new_maps = new_maps\n",
    "        self.pooling_width = pooling_width\n",
    "        super(FGCNNLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv_layers = []\n",
    "        self.pooling_layers = []\n",
    "        self.dense_layers = []\n",
    "        pooling_shape = input_shape.as_list() + [1, ]\n",
    "        embedding_size = int(input_shape[-1])\n",
    "        for i in range(len(self.filters)):\n",
    "            filters = self.filters[i]\n",
    "            width = self.kernel_width[i]\n",
    "            new_filters = self.new_maps[i]\n",
    "            pooling_width = self.pooling_width[i]\n",
    "            conv_output_shape = self._conv_output_shape(\n",
    "                pooling_shape, (width, 1))\n",
    "            pooling_shape = self._pooling_output_shape(\n",
    "                conv_output_shape, (pooling_width, 1))\n",
    "            self.conv_layers.append(Conv2D(filters=filters, \n",
    "                                           kernel_size=(width, 1), \n",
    "                                           strides=(1, 1),\n",
    "                                           padding='same',\n",
    "                                           activation='tanh', \n",
    "                                           use_bias=True, ))\n",
    "            self.pooling_layers.append(MaxPooling2D(pool_size=(pooling_width, 1)))\n",
    "            self.dense_layers.append(Dense(pooling_shape[1] * embedding_size * new_filters,\n",
    "                                           activation='tanh', use_bias=True))\n",
    "        self.flatten = Flatten()\n",
    "        super(FGCNNLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        embedding_size = int(inputs.shape[-1])\n",
    "        pooling_result = tf.expand_dims(inputs, axis=3)\n",
    "        new_feature_list = []\n",
    "\n",
    "        for i in range(1, len(self.filters) + 1):\n",
    "            new_filters = self.new_maps[i - 1]\n",
    "            conv_result = self.conv_layers[i - 1](pooling_result)\n",
    "            pooling_result = self.pooling_layers[i - 1](conv_result)\n",
    "            flatten_result = self.flatten(pooling_result)\n",
    "            new_result = self.dense_layers[i - 1](flatten_result)\n",
    "            new_feature_list.append(\n",
    "                tf.reshape(new_result, (-1, int(pooling_result.shape[1]) * new_filters, embedding_size)))\n",
    "\n",
    "        new_features = tf.concat(new_feature_list, axis=1)\n",
    "        return new_features\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        new_features_num = 0\n",
    "        features_num = input_shape[1]\n",
    "        for i in range(0, len(self.kernel_width)):\n",
    "            pooled_features_num = features_num // self.pooling_width[i]\n",
    "            new_features_num += self.new_maps[i] * pooled_features_num\n",
    "            features_num = pooled_features_num\n",
    "        return (None, new_features_num, input_shape[-1])\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'kernel_width': self.kernel_width, 'filters': self.filters, 'new_maps': self.new_maps,\n",
    "                  'pooling_width': self.pooling_width}\n",
    "        base_config = super(FGCNNLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def _conv_output_shape(self, input_shape, kernel_size):\n",
    "        space = input_shape[1:-1]\n",
    "        new_space = []\n",
    "        for i in range(len(space)):\n",
    "            new_dim = conv_output_length(\n",
    "                space[i],\n",
    "                kernel_size[i],\n",
    "                padding='same',\n",
    "                stride=1,\n",
    "                dilation=1)\n",
    "            new_space.append(new_dim)\n",
    "        return ([input_shape[0]] + new_space + [self.filters])\n",
    "\n",
    "    def _pooling_output_shape(self, input_shape, pool_size):\n",
    "        rows = input_shape[1]\n",
    "        cols = input_shape[2]\n",
    "        rows = conv_output_length(rows, pool_size[0], 'valid', pool_size[0])\n",
    "        cols = conv_output_length(cols, pool_size[1], 'valid', pool_size[1])\n",
    "        return [input_shape[0], rows, cols, input_shape[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple attention\n",
    "# softmax(tanh(qW)) * q\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, seq_len, embedding_dims]\n",
    "\n",
    "    Output shape\n",
    "    - [batch_size, embedding_dims]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = 0\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.step_dim = int(input_shape[1])\n",
    "        self.features_dim = int(input_shape[-1])\n",
    "\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(self.features_dim,),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(self.step_dim,),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        # inputs: [batch_size, seq_len, features_dim]\n",
    "        # eij = [batch_size * seq_len, features_dim] dot [features_dim, 1] = [batch_size * seq_len, 1]\n",
    "        # eij = [batch_size, seq_len]\n",
    "        eij = K.dot(K.reshape(inputs, (-1, features_dim)), K.reshape(self.W, (features_dim, 1)))\n",
    "        eij = K.reshape(eij, (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # [batch_size, seq_len]\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        # [batch_size, seq_len, features_dim] * [batch_size, seq_len, 1] \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = inputs * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoint\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, field_nums, embedding_size]\n",
    "    Output shape\n",
    "    - [batch_size, field_nums, att_embedding_size * head_num]\n",
    "\"\"\"\n",
    "\n",
    "class InteractingLayer(Layer):\n",
    "    def __init__(self, att_embedding_size=8, head_num=2, use_res=True, **kwargs):\n",
    "        self.att_embedding_size = att_embedding_size\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        super(InteractingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_size = int(input_shape[-1])\n",
    "        self.W_Query = self.add_weight(name='query', \n",
    "                                       shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        self.W_key = self.add_weight(name='key', \n",
    "                                     shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        self.W_Value = self.add_weight(name='value', \n",
    "                                       shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        if self.use_res:\n",
    "            self.W_Res = self.add_weight(name='res', \n",
    "                                         shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        super(InteractingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        querys = tf.tensordot(inputs, self.W_Query, axes=(-1, 0))\n",
    "        keys = tf.tensordot(inputs, self.W_key, axes=(-1, 0))\n",
    "        values = tf.tensordot(inputs, self.W_Value, axes=(-1, 0))\n",
    "\n",
    "        querys = tf.stack(tf.split(querys, self.head_num, axis=2))\n",
    "        keys = tf.stack(tf.split(keys, self.head_num, axis=2))\n",
    "        values = tf.stack(tf.split(values, self.head_num, axis=2))\n",
    "\n",
    "        inner_product = tf.matmul(querys, keys, transpose_b=True)\n",
    "        self.normalized_att_scores = tf.nn.softmax(inner_product)\n",
    "\n",
    "        result = tf.matmul(self.normalized_att_scores, values)\n",
    "        result = tf.concat(tf.split(result, self.head_num, ), axis=-1)\n",
    "        result = tf.squeeze(result, axis=0)\n",
    "\n",
    "        if self.use_res:\n",
    "            result += tf.tensordot(inputs, self.W_Res, axes=(-1, 0))\n",
    "        result = tf.nn.relu(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[1], self.att_embedding_size * self.head_num)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res}\n",
    "        base_config = super(InteractingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capsule\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, field_nums, embedding_size]\n",
    "    Output shape\n",
    "    - [batch_size, num_capsule, dim_capsule]\n",
    "\"\"\"\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = int(input_shape[-1])\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = int(input_shape[-2])\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        #动态路由部分\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFM\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - A list of [batch_size, 1, embedding_size]\n",
    "    Output shape\n",
    "    - [batch_size, 1]\n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "\n",
    "class AFMLayer(Layer):\n",
    "    def __init__(self, attention_factor=4, **kwargs):\n",
    "        self.attention_factor = attention_factor\n",
    "        super(AFMLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape_set = set()\n",
    "        reduced_input_shape = [shape.as_list() for shape in input_shape]\n",
    "        for i in range(len(input_shape)):\n",
    "            shape_set.add(tuple(reduced_input_shape[i]))\n",
    "\n",
    "        embedding_size = int(input_shape[0][-1])\n",
    "\n",
    "        self.attention_W = self.add_weight(shape=(embedding_size, self.attention_factor), \n",
    "                                           initializer=glorot_normal(),\n",
    "                                           name=\"attention_W\")\n",
    "        self.attention_b = self.add_weight(shape=(self.attention_factor,), \n",
    "                                           initializer=Zeros(), \n",
    "                                           name=\"attention_b\")\n",
    "        self.projection_h = self.add_weight(shape=(self.attention_factor, 1),\n",
    "                                            initializer=glorot_normal(), \n",
    "                                            name=\"projection_h\")\n",
    "        self.projection_p = self.add_weight(shape=(embedding_size, 1), \n",
    "                                            initializer=glorot_normal(), \n",
    "                                            name=\"projection_p\")\n",
    "        self.tensordot = tf.keras.layers.Lambda(\n",
    "            lambda x: tf.tensordot(x[0], x[1], axes=(-1, 0)))\n",
    "        super(AFMLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        embeds_vec_list = inputs\n",
    "        row = []\n",
    "        col = []\n",
    "\n",
    "        for r, c in itertools.combinations(embeds_vec_list, 2):\n",
    "            row.append(r)\n",
    "            col.append(c)\n",
    "\n",
    "        p = tf.concat(row, axis=1)\n",
    "        q = tf.concat(col, axis=1)\n",
    "        inner_product = p * q\n",
    "\n",
    "        bi_interaction = inner_product\n",
    "        attention_temp = tf.nn.relu(tf.nn.bias_add(tf.tensordot(\n",
    "            bi_interaction, self.attention_W, axes=(-1, 0)), self.attention_b))\n",
    "        \n",
    "        self.normalized_att_score = tf.nn.softmax(tf.tensordot(\n",
    "            attention_temp, self.projection_h, axes=(-1, 0)), dim=1)\n",
    "        attention_output = tf.reduce_sum(\n",
    "            self.normalized_att_score * bi_interaction, axis=1)\n",
    "\n",
    "        afm_out = self.tensordot([attention_output, self.projection_p])\n",
    "        return afm_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'attention_factor': self.attention_factor}\n",
    "        base_config = super(AFMLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMoE\n",
    "\n",
    "\"\"\"\n",
    "    Input shape\n",
    "    - [batch_size, feature_dims]\n",
    "    Output shape\n",
    "    - task_nums * [batch_size, hidden_units]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class MMoE(Layer):\n",
    "    def __init__(self, hidden_units=64, expert_nums=4, task_nums=2, **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.expert_nums = expert_nums\n",
    "        self.task_nums = task_nums\n",
    "        super(MMoE, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.expert_layers = [Dense(self.hidden_units, activation='relu') for _ in range(self.expert_nums)]\n",
    "        self.gate_layers = [Dense(self.expert_nums, activation='softmax') for _ in range(self.task_nums)]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        expert_outputs, gate_outputs, final_outputs = [], [], []\n",
    "        for expert_layer in self.expert_layers:\n",
    "            expert_output = tf.expand_dims(expert_layer(inputs), axis=2)\n",
    "            expert_outputs.append(expert_output)\n",
    "        expert_outputs = tf.concat(expert_outputs, axis=2)\n",
    "\n",
    "        for gate_layer in self.gate_layers:\n",
    "            gate_outputs.append(gate_layer(inputs))\n",
    "\n",
    "        for gate_output in gate_outputs:\n",
    "            expanded_gate_output = tf.expand_dims(gate_output, axis=1)\n",
    "            weighted_expert_output = expert_outputs * expanded_gate_output\n",
    "            task_output = tf.reduce_sum(weighted_expert_output, axis=2)\n",
    "            final_outputs.append(task_output)\n",
    "        \n",
    "        return final_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
